{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import py_entitymatching as em\n",
    "import os, sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasets directory\n",
    "#datasets_dir = \"/home/zeus/data_science/datasets/datasets\"\n",
    "datasets_dir = \"/Users/nehamittal/Downloads/datasets\"\n",
    "#input tables\n",
    "path_songs = datasets_dir + os.sep +'songs.csv'\n",
    "path_tracks= datasets_dir+ os.sep + \"tracks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"py_entitymatching.io.parsers\"\n"
     ]
    }
   ],
   "source": [
    "songs = em.read_csv_metadata(path_songs, key='id',low_memory=False)\n",
    "tracks = em.read_csv_metadata(path_tracks, key='id',low_memory=False)\n",
    "#schema of both songs and tracks- id,title,artist,year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have labeled the data and stored it in \n",
    "# sampled.csv\n",
    "path_labeled_data = datasets_dir + os.sep + 'sampled.csv'\n",
    "G = em.read_csv_metadata(path_labeled_data,key='_id',\n",
    "                         ltable=songs, rtable=tracks,\n",
    "                         fk_ltable='ltable_id', fk_rtable='rtable_id'\n",
    "                         )\n",
    "G['key'] = range(0, len(G))\n",
    "em.set_key(G, 'key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split S into I an J\n",
    "#em.get_key(G)\n",
    "IJ = em.split_train_test(G, train_proportion=0.7, random_state=0)\n",
    "I = IJ['train']\n",
    "J = IJ['test']\n",
    "#I.to_csv(datasets_dir + \"/\" +\"SetI.csv\")\n",
    "#J.to_csv(datasets_dir + \"/\" +\"SetJ.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a set of ML-matchers\n",
    "dt = em.DTMatcher(name='DecisionTree', random_state=0)\n",
    "svm = em.SVMMatcher(name='SVM', random_state=0)\n",
    "rf = em.RFMatcher(name='RF', random_state=0)\n",
    "lg = em.LogRegMatcher(name='LogReg', random_state=0)\n",
    "ln = em.LinRegMatcher(name='LinReg')\n",
    "nb = em.NBMatcher(name=\"NaiveBayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             id_id_exm\n",
       "1                             id_id_anm\n",
       "2                        id_id_lev_dist\n",
       "3                         id_id_lev_sim\n",
       "4           title_title_jac_qgm_3_qgm_3\n",
       "5       title_title_cos_dlm_dc0_dlm_dc0\n",
       "6       title_title_jac_dlm_dc0_dlm_dc0\n",
       "7                       title_title_mel\n",
       "8                  title_title_lev_dist\n",
       "9                   title_title_lev_sim\n",
       "10                      title_title_nmw\n",
       "11                       title_title_sw\n",
       "12        artist_artist_jac_qgm_3_qgm_3\n",
       "13    artist_artist_cos_dlm_dc0_dlm_dc0\n",
       "14    artist_artist_jac_dlm_dc0_dlm_dc0\n",
       "15                    artist_artist_mel\n",
       "16               artist_artist_lev_dist\n",
       "17                artist_artist_lev_sim\n",
       "18                    artist_artist_nmw\n",
       "19                     artist_artist_sw\n",
       "20                        year_year_exm\n",
       "21                        year_year_anm\n",
       "22                   year_year_lev_dist\n",
       "23                    year_year_lev_sim\n",
       "Name: feature_name, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a set of features\n",
    "F = em.get_features_for_matching(songs, tracks)\n",
    "F.feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the I into a set of feature vectors using F\n",
    "H = em.extract_feature_vecs(I, \n",
    "                            feature_table=F, \n",
    "                            attrs_after='gold_label',\n",
    "                            show_progress=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the feature vectors contain missing values\n",
    "# A return value of True means that there are missing values\n",
    "any(pd.notnull(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Impute feature vectors with the mean of the column values.\n",
    "H = em.impute_table(H, \n",
    "                exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'],\n",
    "                strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select the best ML matcher using CV\n",
    "result = em.select_matcher([dt, rf, svm, ln, lg,nb], table=H, \n",
    "        exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'],\n",
    "        k=5,\n",
    "        target_attr='gold_label', metric='f1', random_state=0)\n",
    "result['cv_stats'].to_csv(datasets_dir + \"/\" +\"F1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select the best ML matcher using CV\n",
    "result = em.select_matcher([dt, rf, svm, ln, lg,nb], table=H, \n",
    "        exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'],\n",
    "        k=5,\n",
    "        target_attr='gold_label', metric='precision', random_state=0)\n",
    "result['cv_stats'].to_csv(datasets_dir + \"/\" +\"precison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select the best ML matcher using CV\n",
    "result = em.select_matcher([dt, rf, svm, ln, lg,nb], table=H, \n",
    "        exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'],\n",
    "        k=5,\n",
    "        target_attr='gold_label', metric='recall', random_state=0)\n",
    "result['cv_stats'].to_csv(datasets_dir + \"/\" +\"recall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split H into P and Q\n",
    "PQ = em.split_train_test(H, train_proportion=0.7, random_state=0)\n",
    "P = PQ['train']\n",
    "Q = PQ['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Debug RF matcher using GUI\n",
    "em.vis_debug_rf(rf, P, Q, \n",
    "        exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'],\n",
    "        target_attr='gold_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train using feature vectors from I \n",
    "rf.fit(table=H, \n",
    "       exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'], \n",
    "       target_attr='gold_label')\n",
    "\n",
    "dt.fit(table=H, \n",
    "       exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'], \n",
    "       target_attr='gold_label')\n",
    "\n",
    "svm.fit(table=H, \n",
    "       exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'], \n",
    "       target_attr='gold_label')\n",
    "\n",
    "ln.fit(table=H, \n",
    "       exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'], \n",
    "       target_attr='gold_label')\n",
    "\n",
    "lg.fit(table=H, \n",
    "       exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'], \n",
    "       target_attr='gold_label')\n",
    "\n",
    "nb.fit(table=H, \n",
    "       exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'], \n",
    "       target_attr='gold_label')\n",
    "\n",
    "# Convert J into a set of feature vectors using F\n",
    "L = em.extract_feature_vecs(J, feature_table=F,\n",
    "                            attrs_after='gold_label', show_progress=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 92.86% (52/56)\n",
      "Recall : 92.86% (52/56)\n",
      "F1 : 92.86%\n",
      "False positives : 4 (out of 56 positive predictions)\n",
      "False negatives : 4 (out of 51 negative predictions)\n"
     ]
    }
   ],
   "source": [
    "# Predict on L \n",
    "predictions = rf.predict(table=L, exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'], \n",
    "              append=True, target_attr='predicted', inplace=False)\n",
    "\n",
    "# Evaluate the predictions\n",
    "eval_result = em.eval_matches(predictions, 'gold_label', 'predicted')\n",
    "em.print_eval_summary(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 87.93% (51/58)\n",
      "Recall : 91.07% (51/56)\n",
      "F1 : 89.47%\n",
      "False positives : 7 (out of 58 positive predictions)\n",
      "False negatives : 5 (out of 49 negative predictions)\n"
     ]
    }
   ],
   "source": [
    "# Predict on L \n",
    "predictions = dt.predict(table=L, exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'], \n",
    "              append=True, target_attr='predicted', inplace=False)\n",
    "\n",
    "# Evaluate the predictions\n",
    "eval_result = em.eval_matches(predictions, 'gold_label', 'predicted')\n",
    "em.print_eval_summary(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 94.64% (53/56)\n",
      "Recall : 94.64% (53/56)\n",
      "F1 : 94.64%\n",
      "False positives : 3 (out of 56 positive predictions)\n",
      "False negatives : 3 (out of 51 negative predictions)\n"
     ]
    }
   ],
   "source": [
    "# Predict on L \n",
    "predictions = svm.predict(table=L, exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'], \n",
    "              append=True, target_attr='predicted', inplace=False)\n",
    "# Evaluate the predictions\n",
    "\n",
    "eval_result = em.eval_matches(predictions, 'gold_label', 'predicted')\n",
    "em.print_eval_summary(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 98.11% (52/53)\n",
      "Recall : 92.86% (52/56)\n",
      "F1 : 95.41%\n",
      "False positives : 1 (out of 53 positive predictions)\n",
      "False negatives : 4 (out of 54 negative predictions)\n"
     ]
    }
   ],
   "source": [
    "# Predict on L \n",
    "predictions = ln.predict(table=L, exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'], \n",
    "              append=True, target_attr='predicted', inplace=False)\n",
    "# Evaluate the predictions\n",
    "eval_result = em.eval_matches(predictions, 'gold_label', 'predicted')\n",
    "em.print_eval_summary(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 94.55% (52/55)\n",
      "Recall : 92.86% (52/56)\n",
      "F1 : 93.69%\n",
      "False positives : 3 (out of 55 positive predictions)\n",
      "False negatives : 4 (out of 52 negative predictions)\n"
     ]
    }
   ],
   "source": [
    "# Predict on L \n",
    "predictions = lg.predict(table=L, exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'], \n",
    "              append=True, target_attr='predicted', inplace=False)\n",
    "# Evaluate the predictions\n",
    "eval_result = em.eval_matches(predictions, 'gold_label', 'predicted')\n",
    "em.print_eval_summary(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 96.15% (50/52)\n",
      "Recall : 89.29% (50/56)\n",
      "F1 : 92.59%\n",
      "False positives : 2 (out of 52 positive predictions)\n",
      "False negatives : 6 (out of 55 negative predictions)\n"
     ]
    }
   ],
   "source": [
    "# Predict on L \n",
    "predictions = nb.predict(table=L, exclude_attrs=['key', 'ltable_id', 'rtable_id', 'gold_label'], \n",
    "              append=True, target_attr='predicted', inplace=False)\n",
    "# Evaluate the predictions\n",
    "eval_result = em.eval_matches(predictions, 'gold_label', 'predicted')\n",
    "em.print_eval_summary(eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
